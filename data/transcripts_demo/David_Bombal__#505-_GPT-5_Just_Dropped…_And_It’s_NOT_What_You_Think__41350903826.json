{
  "podcast": "David Bombal",
  "episode_id": 41350903826,
  "episode_title": "#505: GPT-5 Just Dropped… And It’s NOT What You Think",
  "audio_url": "https://feeds.soundcloud.com/stream/2154910707-davidbombal-505-gpt-5-just-dropped-and-its-not-what-you-think.mp3",
  "duration_seconds": 2134.541,
  "created_at": 1755526852,
  "podcast_author": "David Bombal",
  "podcast_description": "Tech podcast by David Bombal featuring cybersecurity, networking, and AI experts.",
  "text": " We don't know how to do it.  You would need very smart systems that are much more reliable than what we have to really  underwrite the agents that people want right now.  And so we're getting like a sneak preview of the future, but it'd be like getting a sneak  preview of a car in 1850 and, you know, one in four of them blows up in your face.  It's like, we should probably wait on that.  Let somebody else test drive those things and they'll, you know, they'll learn that gasoline  is explosive and that, you know, maybe you shouldn't be in the vehicle when you fuel  and stuff like that, which people  would learn to vote.  Apple actually had in their system  from about a year ago, maybe still do.  Told the systems, be truthful, don't make stuff up.  Don't hallucinate.  And then they did anyway.  That's a simple version of alignment.  I'm going to tell you, don't make stuff up.  And I could tell that to an intern working for me,  an employee.  And if I told that to an employee who was working for me  and they kept doing it, they'd be gone.  Exactly.  But LLMs instead of being gone, we're  giving them another $500 billion in funding.  It's kind of insane.  Everyone, David Bumble, coming to you from Black Hat  with a very, very special guest.  Gary, welcome.  Thanks for having me.  Great to meet you.  My team have been telling me so much  about what you've been sharing  and I've been watching your videos and stuff.  So happy to have you on the show,  giving us a reality check about AI.  But before we even get there, something big has just dropped.  It's very just minutes, literally, before I came.  GPT-5 Open AI dropped GPT-5.  They did a public announcement of GPT-5.  And I have to say, for most people,  it's probably disappointing.  People have been talking about GPT-5  since GPT-4 came out.  That was 32 months ago.  And in fact, Bill Gates saw a preview of GPT-4  four years ago if I have my math correctly.  No, three years ago.  Three full years ago.  And ever since then, people have been like,  wow, GPT-5 is gonna be amazing.  In fact, I just tweeted about it.  picture with Kevin Scott, his Microsoft CTO, with a little picture of GPT-3 is some tiny little  sea creature. I guess it's a tiny little whale. And then GPT-4 is a little bigger, but  noticeably bigger. And then GPT-5 in this diagram that Kevin Scott did about a year ago is his giant  humpback whale. And what I've been saying all along is it's not really going to be this great  humpback whale. It's going to be like the other systems a little bit better, but not hugely better.  And that's kind of what we saw. GPT-5 is better on many benchmarks. The biggest thing about today's  presentation, they already got nailed at minutes into the presentation, which is they showed all  these benchmarks and they say, we're better on this, we're better than that. Sam Altman says,  this is the smartest model of all time, but they didn't report Francois Scholé's  Arc AGI-2, which is a benchmark a lot of people pay attention to and that they pay attention to  in there, about three and demo six months ago.  Didn't mention it.  Well, guess what?  It doesn't even beat Grock four,  which came out a month or two ago.  So we have this story we keep hearing  about exponential progress.  Someday I know vaguely from Twitter posted today.  I don't wanna hear about exponential progress anymore.  This is someone who is enthusiastic  and kind of believing that it has become disillusioned.  I think a lot of people are slowly realizing  that they've been sold to Bill of Goods.  It's one thing to say each model's gonna be  a little bit better than the last.  Well of course it's going to be you spend a billion dollars on the model, you should get some return on your investment  mutuals up, but that doesn't mean that these things are AGI, artificial general intelligence.  This new model, definitely not AGI, can't even be crock on this one test.  There's incremental progress here, there is nothing like this giant humpback whale leap that we've been promised.  Big thanks to Brilliant for sponsoring this video.  Have you got a crazy schedule but want to learn something new every day?  Well, I've got an app for that and that's the brilliant platform.  You can have access to the brilliant platform by installing an app on your phone,  which gives you the freedom to study wherever you are.  Doesn't matter if you're commuting on the train or if you just have a few  minutes downtime, the brilliant app allows you to learn something in bite-sized  intervals when you've only got a few minutes of downtime.  Don't just look at mindless content where you won't even remember what you've  looked at tomorrow.  rather spend your time learning something.  But it's also really important to be consistent  in your learning.  Learn a little bit every day,  and Brilliant can help you with this,  with their app.  As an example, you can build your Python programming skills  with a Python with functions course.  You start with a basics, a quick introduction to functions,  and then you'll learn about tuples or tuples,  if you prefer, how to debug functions,  what function dependency is,  and how you should compose a function,  you'll even learn about multiple inputs for a function.  And then brilliant challenges you do go further  so that you can hit those big learning goals one day at a time.  And there you go, consistent learning,  or locked in with an app on your phone.  If you want to try everything brilliant has to offer,  go to brilliant.org forward slash David Bumble  or you can scan the QR code on screen.  You'll also get 20% off an annual premium subscription.  Make sure that you use my link in the video description  and start learning something new today.  Education changes lives.  You can change your life by just studying  a little bit every day.  Because I keep hearing the hype, right?  AI, this AI that everyone's got AI on every product.  But it seems just to be a lot of hype, a lot of the time.  I mean, there's real progress.  So, you know, the systems today  are definitely better than systems three years ago.  But the hype is like this is going to solve all your problems.  Reality is there are some context for it.  I mean, what we learn at this conference  is that the biggest context for it,  which is coding, actually, is huge flaw.  So for the last year, everybody's been talking about agents  and how great they're going to be.  I kept saying, come on, hold your horses.  They're going to be flawed because they have multiple steps.  Every individual step with an L.M. is actually a risk.  Something's going to go wrong.  So you put them together, get serious problems.  And there were two talks here, one from NVIDIA,  another this morning from Nathan Hamill,  showing that these things called prompt injection attacks  make these systems like total sitting docks.  So if you use one of these so-called  level three agents as NVIDIA's terminology is,  it goes out in the web.  It looks for examples.  And those examples, I think they call them like  watering wells or something like that,  are just filled or can be filled with bad code  that does terrible thing.  So they show basically live, or not quite live demos,  but recordings of demos where they would ask a system  to create this bit of code.  In next thing you know, someone basically  had control of their system, remote code execution,  which is basically the worst thing they can happen to you.  And they showed not one, but a dozen different ways  to do this.  And each of those ways is actually some huge attack surface.  So essentially any code that you can get the systems to absorb  becomes an attack surface.  And so those are just huge problems here.  One way to think about is we want our AI to be safe and secure helpful and so forth.  That means we want to be able to instruct it.  We want to tell it stuff.  So the funniest in an ironic dark sense of funny dark, dark humor funniest thing in this  talk is somebody put in or they put in in one of the like basically like a system product,  user prompt for the coding, they put in only right secure code.  This is to have no idea what that actually means, right?  It's like when you tell the system to do this bank transaction for me and it says, I'm  on it and it doesn't actually know your passwords and so it can't do it.  It's just spouting things that people say, if you asked it, are you writing secure code?  It might say that it is, but the reality is it's not.  It doesn't have a deep enough conceptual understanding of what secure code is.  And the same is true in any domain.  So we've seen it delete people's databases and all that.  Yeah, I'm glad you mentioned that because we had black hats.  And I mean, my big concern with this is the cybersecurity piece.  I'm glad you mentioned agents because if you got agents supposedly doing everything autonomously,  they're not riding secure code.  I mean, it's an attack as paradise.  You know, maybe 20 years from now, agents will write really great code.  There'll be much better than humans.  But we're going to be in a transitional period for a while where agents are going to write  shit code that is not really secure.  Is not as good as the best coders can do.  Partly because the best coders aren't like taking everything off random repositories that  they don't understand.  And they understand the code that they're using.  They're good.  I mean, bad coders have always been a problem, but good coders have a conceptual understanding  of, you know, what a memory overflow error is.  And they look at some piece of obfuscated code.  They're like, I'm not putting that in.  I don't know what that does.  It looks like a bunch of digits because it's, you know, been obfuscated and they know  better than to use the code.  But these systems, these agents systems don't really know what to use.  And so it's just, they're sprinkled with so many vulnerabilities.  And if you use these systems only to say, hey, what's a way I could write a piece of code  for some small little thing?  Like, I want to learn how to translate this into bold and Unicode.  Right.  You can look that up.  You could have looked it up on Stack Overflow before.  Now you can use ChatTPT or Cloud or whatever to find that piece of code for you.  That's fine.  That's not what I mean by full agency.  Full agency is like, I have bug reports on my code.  Go fix them.  It's not me finding one little piece of code that I with my human eyes can check out.  That's like leaving the system with a lot of autonomy.  And it's really when that autonomy comes in that you have problems.  I know you were just interviewing, I want to get his name right, Michael Hippone and I saw  his talk yesterday.  And what's the title of his book?  If you make it smart, it's going to be vulnerable or something like that.  It's if it's small, it's vulnerable.  It's smart, it's vulnerable.  Well, that is true in spades with an asterisk in this situation.  These things are not really smart, but the given powers, right?  It's really, if it's autonomous, it becomes vulnerable.  And some of the vulnerability comes from stupidity.  So it doesn't understand, for example, that if somebody writes something in white on white  text, that's probably actually bad news.  They probably get it to conceal it from your system, right?  They don't really understand that.  And, you know, people will add band-aids to this and to that.  But there's so many ways to attack and coach up.  If you're talking about my code, because I write a video game to amuse my kids, probably  Nobody's gonna go after me.  But if you're talking about defense department stuff,  or whatever, then somebody wants to get in,  they're gonna be able to find many ways to get in.  I mean, it's already the state of cybersecurity  as you well know is not great,  that it's gonna get much, much worse.  There's all of that.  Yeah, I mean, you mentioned agents,  but I'm assuming you're also doing about vlog coding.  That's what we're talking about.  So the vlog coding uses it.  So, I mean, vlog coding is basically subject to this,  people use different tools.  And when they're doing this vlog coding stuff  that got some agent in the background  with some degree of autonomy.  One of the things that was mentioned in the video talk was  there's actually something you can switch off  called auto install or I figure what is,  what the name of the option is.  But basically you have the system right code  and like if it's a package it downloads,  it runs the code for you.  And that's probably the worst thing.  And now like the people making vibe coding platforms  are like at least giving you an option to turn it off  or maybe free or all company.  But what happens is a little bit,  we're in Vegas, it's a little bit like slot machines.  is people will just keep pulling the handle  and they're just gonna keep looking.  So even if that option is switched off,  they're gonna get a lot of like,  do you wanna install this package?  And their eyes are gonna glaze over,  they wanna move fast  cause the, you know, coders get rewarded for how much code  they're gonna end up installing these packages  anyway, whether it's auto installed or not.  And it's when you install code from the outside  that you don't understand.  There was another example where there was code in a comment  And it was like hidden to the right of the screen.  And so like the user's not even gonna notice this code  off of the right of screen.  They're looking here and it's over there.  Like so much stuff is gonna filter in.  And yes, a lot of that is in the vibe coding context.  Yeah, I mean, it just seems amazing that they wanna give  so much power to agents to run our lives.  And it's not gonna actually just question, right?  We're just not ready for that.  Yeah.  It's something we should aspire to, I suppose.  Like, you know, someday we will want that.  I mean, just like, you know, instead of hiring somebody,  If I can just do this and say, it's money, maybe it's faster, I don't have to worry about  what time zone they're in.  Like the reason is why you might want this.  But the problem is the only way we know how to build these things right now is with  LLMs.  And large language models are inherently flawed.  They hallucinate and make weird errors of reasoning.  I mean, imagine if they hallucinate code.  And in fact, some people are looking into that as a vulnerability.  Like you can guess what the hallucinations will be.  Instead of putting the offending code in the clear, you put code that's similar enough  that there will be a loose nation that will be the thing that you want.  How are you going to defend against that?  We don't know how to do it.  You would need very smart systems that are much more reliable than what we have to really  underwrite the agents that people want right now.  So we're getting like a sneak preview of the future, but it'd be like getting a sneak  preview of a car in 1850 and one in four of them blows up in your face.  It's like, we should probably wait on that.  Let somebody else test drive those things and they'll learn the gasoline.  lean is explosive and that maybe you shouldn't be in the vehicle when you fuel it and  stuff like that, which like people that should learn in boats.  We're such early days and there are dangers and the dangers here are not that anybody's  directly going to die, but people may indirectly die because you have bad code that controls  an electrical grid or nuclear grid or whatever.  And then databases are getting deleted or they get rewritten with who knows what or information  will travel to our enemies and so forth.  There are a lot of real dangers, even if it's not a perfect analogy to a car explosion.  There are going to be lots of problems.  You mentioned the word hallucinations.  Didn't you mention that in your book?  Yeah, I actually think I kind of came up with the idea of hallucination.  This is my 2001 book, The Algebraic Mind.  I really think I laid out most of what we're facing right now in terms of what are the  challenges to a neural network approach.  I was working with earlier neural networks in my dissertation.  This kind of grew out of my dissertation.  My dissertation was about eight years earlier.  a grew out of explorations of the neural networks  that people were building back in those days.  And I did experiments on them and found kind of problems  with them.  And there's this one little passage  that really anticipated hallucinations.  I gave this example of my honest or winning the lottery.  Or I wish that she had.  But so the hypothetical my honest or wins the lottery.  And then I teach that to this neural network.  But the neural network erroneously generalized  the winning of the lottery to 11 of the remaining 15 people  that sort of shared some properties with my answer.  And so I went on to say, this kind of overgeneralization  is the inevitable downside of the kind of automatic  generalization that we're talking about here.  That was basically in 2001, pointing out  the problem of hallucinations and saying it's inevitable  and as long as we're doing this and not something else  and we can talk about with the something else.  Yeah, I'd like you to, because the question is like,  why do LLM solution at?  So they lose any because of what I'm talking about.  Automatic overgeneralization, I guess I haven't  spelled it out clearly enough.  But what they were doing and what we still do  is we have what we call distributed representation.  So instead of having a single database record,  let's say, for each individual person  and then facts about those people,  what neural networks do,  and it helps them when it helps them,  but it hurts them when it hurts them, so it's be,  is it breaks all the information  into little bits of information.  So in this particular system, there was no unique node  for my on-distor, but rather on-distor is coded  as the properties that she shares with other people.  So things weren't a little differently now,  but we talk about embeddings and embedding spaces  and so forth.  And so what you'll find is like a bunch of people  that share properties are put in a similar place.  And then the system guesses that things that might be true  of that cluster of people might be true in general.  In a recent substock, I talked about my friend Harry Shearer,  who you might know, it is a voice for about the Simpsons  character, and he was the bass player in the Simpsons,  and he's been in a bunch of movies.  He's been in a bunch of movies, for example,  with Christopher Guest, who's also in Spinal Tap and so forth.  And so the system, somebody sent a biography to Harry, which he sent to me, said that Harry  Shearer was a British entertainer, voice actor, etc.  But he's not.  He's American.  He's born in Los Angeles and so he thought this was funny.  Then it went on to say that he was in the mood or he helped do voices for Jaws, which  he didn't really do.  And it said a bunch of things were true.  It got the name of his character wrong.  And so like, some of it was true.  You had a chicken, right?  Well, there's a story about me and somebody sent me a biography of me.  I don't sit there asking for biographies from J.P.T. because I think they're worthless,  but somebody else did a biography of me.  They said to me, and it said that I own a pet chicken named Henrietta.  I don't own a pet chicken.  If I did, I wouldn't call it Henrietta.  So this has become kind of a running joke.  And in fact, that's why Harry sheers entered me.  In fact, his title line before he passed someone else's message with the biography was no  Henrietta, but still that's what Harry wrote in his message.  alluding to my famous hallucinatory chicken name Henry.  So this has been a consistent problem going back to 2001.  What really kind of irritates me about the kind of high P people in this field is they're  always saying, yeah, wait till next year.  We'll solve it next year.  I'm like, dude, you've been telling me that for a quarter century and your approach does  not work.  It still has this problem.  Today's models did a little better on some benchmarks with hallucinations, but it's still  like 3%.  I mean, like making stuff up like one in 30 times is really not good stuff that you can easily get  out of Wikipedia and databases. I mean, we're not talking about making stuff up in some,  you know, really obscure thing, but also if you're making it up, you should say so, right? A good  AI system in this comes back to agents should say what it doesn't understand. And so it should  either say Harry Schreier was born in Los Angeles, which is not hard to find, right? You can find  that in Wikipedia on the front page of his entry, the first page of his entry.  Like either it should give you that or at least it should say, I don't know, which it's  say for some reason I feel like telling you London, but I can't bear that out.  Everybody imagines LLMs to be intelligent like other humans.  And you're not.  They're doing something different.  They're doing autocomplete, actually, coin the phrase autocomplete on steroids a number  of years ago.  They're doing autocomplete on steroids.  And that's just what they do.  they don't do fact checking. You know, they can't get a job in the New York or doing fact checking.  That's not what they do.  Artificial intelligence, it's supposed to be some kind of intelligence, but AI is on  intelligence. Is that right to say?  That's a definition old thing. So, you know, you could say that a chess computer, although  actually, I don't know about GPT five, but O3 is still has problems with tests we'll get to in  the second maybe. You could say a chess computer has a measure of intelligence because you could say  intelligence is multi-dimensional. There are many aspects of it. One is being able to do  the raw calculation in chess. And Gary Kasparov was really good at chess. He can do a lot of  other things too. He was really good at politics as well. But a chess computer has a piece of  intelligence. I would say no chess computer, even the one that beat Gary Kasparov is remotely  as intelligent as Kasparov, because Kasparov has this flexibility in how he can reason and think,  and so forth. I actually have a podcast coming out with him any day. And you know, he can, we  talked as fluently about politics as we did about AI and as we could have about chas and you know,  we did talk a little bit about chess. You know, his mind is versatile and not everybody is  Gary Kesparo. In fact, nobody is Gary Kesparo. But even the average human being can actually  fluidly think a lot about a lot of things. So for example, they might go to a Harry Potter movie,  read a Harry Potter book and say, oh, there's this thing called quidditch. And then they can learn  about it, even though it doesn't happen in our world, you know, you don't have a lot of experience  with quidditch before you watch or see Harry Potter because it's not a real world thing, but you  start to understand, okay, well, they're flying around on the broomsticks and this might be  possible. And this might, you know, if I said, could you fly the broomstick in one minute from  England to the United States? You'd say, no, you know, you can start to reason about it,  even though it's a domain that's not familiar. And the current system to have a lot of trouble  doing that. They might be able to do it with quidditch because there's enough data about it.  But if we came up with some other novel thing, they're going to have trouble with it.  So I mean, will all of them ever stop hallucinating? Because you said there's another way to do  it perhaps or something. Well, I think what we have to do, let's say, set them aside. I think I  want to have some value of the really good at learning statistical distributions. And that's  very useful, but they're not good at learning abstractions. They're not good at representing  particular individual bits of information in a reliable way. Like where is Harry Shearer from?  Yeah. Yeah. Now your listeners having heard me say it a few times can probably say Los Angeles,  right? The system can't reliably do that from, you know, even like one of the reasons they use  Harry Shearer is an example is he's famous, right? You know, he's he's not as famous as Dom Trump,  but he's pretty well known. And so there are many entries about him. There's Wikipedia and then  IMDB and, you know, he's been in any number of like profiles and this sort of says a lot of data  there and still it doesn't stick. I meant to talk to you about chess. By the way, which is these models get trained on  just they see lots of games. They see the rules of just a download sites like chest.com that you know,  teach you. And you can ask them a question like can a queen jump over a night and they'll say no and then in the  course of a game they'll actually have a queen jump over a night. So they don't know what's an illegal move. Right? You know, they've been told it.  They can have it an explicit knowledge, but they can't act on it. And that's, you know, this is the same problem  I was talking about with the agent.  You can say right secure code, but they can't really act on that.  Yeah, because I mean, you say it's like a, it's order compute on steroids.  It doesn't.  It doesn't seem to have intelligence and what it's doing.  Again, like just nail that down.  So there are aspects of intelligence that you could say that, but it's certainly not like  a complete thorough intelligence.  It's like, it's almost like we don't really use this term as much anymore, but in  the idiots of on that can like tell you what day a certain calendar day is, which by the way,  when these systems just messed up, a friend might ask what they like September 4th would be  in and get the cut the wrong day, what day, the weekend got wrong.  But so like in it, it's a fun, they can do certain things really well.  In particular, statistical prediction of what is likely in a context, they do extremely well,  but there are other things like fact checking that they can't do it all.  And you would expect a real intelligence to be able or a robust intelligence, let's say,  to do it. So you could say, you know, it's sort of like a bullshitter in, you know,  bar with, you know, sophomore in college trying to fake their way through things. Like, it  can convince the uninitiated, but you know, like anybody who looks in their field of expertise  looks at it and is like, well, when it talks to me about physics, it doesn't really  know about physics. And we have this illusion, well, I don't know any physics, it looks  pretty good to me, but you ask an expert in the experts, like, no, it doesn't actually  understand because it can pair it back a bunch of things that are there.  And they can train them on massive numbers of problems.  So it can solve problems of certain sorts.  But the flexibility and generality of human intelligence, they really lack.  So are we going to see this diminishing returns?  Because I mean, come to you, they're spending huge amounts of money  throwing GPUs, data, whatever, at these things.  Is it we're not going to get this masses spring forward?  Like we were supposed to get with GPT-5?  We were already getting diminishing returns.  And so, you know, GPT-5 is better than GPT-4,  but it's not massively better.  It's measurably better, I would say.  But it's not massively better.  We are seeing a moment of diminishing returns.  We're finding other techniques might help.  One of which is to add classic symbolic techniques.  So the whole history of AI has been  that the neural network people and the symbolic people  basically hate each other.  Can you explain that?  Give us some details about that.  Go on, sir.  Yeah. So the neural networks are what's popular right now.  Now, they try to break bits of information  into tiny little bits, get lots of statistical information,  into things like autocomplete.  The symbolic stuff looks more like classical computer programs,  like algebra, like logic, and we use those for certain things,  like GPS navigation systems, use symbolic AI all the time  and use it very effectively.  Because of fights over resources, like graduate students  and funding and stuff like that,  the two groups have been really hostile to one another.  And what I called for in this book,  the algebraic mind.  The subtitle was integrating connectionism  and cognitive science.  Connectionism and neural network  and cognitive science was more  of the classic, similar manipulation stuff.  That's what we need is some reconciliation,  which I've been begging the field to do for 25 years  and a little bit of it is kind of sneaking  in the back door right now.  They're starting to add some symbolic stuff  into the large language models,  which I think is a little bit too little too late,  but that's where the progress  that they're actually getting is coming from.  So Gary, we had Blackhead again.  I look at this from a sub security point of view.  But what about AI safety?  What can you tell us about that?  I mean, obviously, the related notions.  So cybersecurity is really about like,  is your code secure, can somebody steal it,  can somebody control it?  And there's like a commercial version of that.  And then there's like a humanity version of that,  which is like, is our species safe  from what can be done with these systems?  And there are different aspects.  So for example, can these systems teach somebody  had to build a biological weapon.  Somebody who couldn't otherwise do it,  teach them beyond what they could get from Google.  And so forth.  And the answer increasingly looks like it might be yes.  There are these fears of so-called rogue AI.  My view is we've made a lot more progress,  not on artificial general intelligence,  but at least some usable stuff,  what we can do with the auto-complete on steroids  and so forth.  We've made real progress there, but on what we call  alignment, getting the machine to do what we want to do,  not so much, like hallucinations and example.  Apple actually had in their system problem about a year ago.  Maybe still do.  Told the systems, be truthful, don't make stuff up.  Don't hallucinate.  And then they did anyway.  That's a simple version of alignment.  I'm going to tell you, don't make stuff up.  And I could tell that to an intern working for me and employ.  And if I told that to an employee who was working for me  and they kept doing it, they'd be gone.  Exactly.  But LLMs instead of being gone, we're  giving them another $500 billion in funding.  It's kind of insane.  If you can't tell the system, don't hallucinate.  What about like, don't cause harm to humans?  Can you really get a system to understand that?  And the answer is no.  And there's been no major progress on that problem in, I don't know, quarter century.  You could argue that the biggest progress was made by Isaac Asimov led in the 1950s when  he proposed knowing that they weren't all that great.  Asimov's laws, what we call it, you know, maybe he called them the three laws of robotics.  They weren't that great because they don't cover all the situations and so forth.  And that was part of the point of his stories is like this is actually hard, but at least  he raised the question of like what ethical principles would you instill a machine?  He raised the right question.  That's mouth did.  We still don't have an answer either to what you should actually code in, which is hard  and you know, their cultural issues and so forth.  But we don't have an answer to that.  And we don't have an answer to how to do that, right?  And you know, bunch of people have worked on that, but they're all working on the LLM context.  My view is, LOMs are too thick.  They'd not semantically rich enough to be able to understand such concepts.  So another way to put all of this is, LOMs are black boxes.  We put a bunch of data in and we get something out and we hope for the best.  That's basically how it works.  We want to run around the world on that.  We should not run the world and hope for the best.  We should really be building white box AI where we understand what's in it so that we can  debug it.  Right? We radically shifted to this black box way of doing this.  Now we're writing code.  Vibe coding is mostly at some level black box code.  It's not literally because you can actually at least see its output.  But from the perspective of many users, it basically is a black.  They don't understand the code that they've written or that they've had a  machine right.  And the LLMs themselves are black boxes.  And it's just not really working when it comes to safety.  It may be working with respect to making money.  even there, the only person really making money is Jensen Huang and his, you know,  his company and video, right? Selling chips, but you know, the analogy at which many people  views at this point is it's like selling shovels in the gold rush. You know, Jensen selling  the shovels, he makes a great shovel. They, you know, they work really well. There's a wonderful  ecosystem around them. He's a great manager, the company. Everything about the company  is great. But the problem is that the people buying this stuff don't actually know how to  to make money from it.  None of them are almost none of them  have actually made any profits.  And that's partly because the stuff is unreliable.  So do you see it just continuing like this?  Are we gonna hit a wall somewhere?  Have we hit a wall?  Well, I wrote a paper in 2022,  saying deep learning is hitting a wall.  And the point of it was that we couldn't just  keep scaling pre-training data.  And in fact, that has proven to be correct.  Part of the argument was not that we would make no progress  at all, but that we would keep running into the same problems,  which were hallucinations and problems with reasoning  and misuse for misinformation and so forth.  And we're still seeing those same obstacles.  You know, maybe I chose the wrong metaphor there  because you can always say,  well, I made progress over here,  but the fact is on the critical dimensions  of how you would make things like reliable agents,  you've not made them as progress.  And I mean, I spoke to someone who promised AGI this year,  what you're taking AGI for when?  Yeah, or is it gonna ever happen?  Or is it reality?  Is it just a...  It'll happen.  You know, there's no principle reason.  It's really an architectural problem. We have a wrong architecture and we don't know how long it will take to find the right architecture.  It's very unlikely that we're going to see AGI in the next couple of years just by doing more of the same.  It's just not really working. I mean, it works a bit, but it's not really getting a stage.  Yeah, it's possible we can get there in five years. It's a relatively long time with this many people doing research and so forth.  You know, it's more likely we get there in 10 years and you know, it's very likely we'll get there in 100 years, but we don't know exactly when.  exactly when.  I'm just going to say I heard the Google talk about something slightly different to that.  I don't think it was Google.  I think it was actually Mustafa at Microsoft.  Okay.  I actually talked about a very similar idea a long time ago, which was point-to-listic  AI, I think like five years ago.  And then Ethan Mollock turned it to Jagged Intelligence and then I think Mustafa borrowed  it.  Nobody getting attribution a long line.  But the idea is that these systems get problems that are, well, my version of the idea is  they get problems that are close to what they've been trained on, but you move further away  and they're less reliable.  And so they are jagged or pointy-listic in the sense of like, they get some of the points,  but not others and you don't really know in advance whether it's going to work.  One of the insane things about this current era is compared to the era of calculators and  spreadsheets.  Calculators and spreadsheets, if you pose the problem correctly, you will get a correct  answer.  you can leave something out in a spreadsheet your formula can be wrong. That's on you. It's not on  the system. Excel will calculate what you tell it to calculate. My pocket calculator will tell you.  A slide rule never hallucinated, not once. I have, you know, it's too young to use them much,  but I did learn how to use them. That is not true of this class of AI products. It's also  not necessary for all AI. So GPS systems don't like invent streets. We're here on the  On the Vegas strip, they don't make an alternate version of the street that goes like the  Lombard Street in San Francisco.  I mean, they just don't do that.  Exactly.  You know, classical AI does not make stuff up.  We're steeped in this crazy paradigm where we're pretending, really, linking.  Hey, we don't mind if we just make stuff up.  But it can't last.  Yeah, I mean, that's one exhibition.  It's like, where do you see this going in the next few years?  Just more money thrown at it.  What do you think is going to happen?  What's your predictions?  I mean, the most famous stock advice that I ever got,  and I don't really play the market,  was the market can stay irrational longer  than you can remain solvent.  So why am I not shorting all of this stuff?  It's because I don't know how long the delusions will last.  I mean, you could have looked at tulips  and said back in the day and said,  the list is insane, but you don't know when people  are gonna stop spending $7,000 on one tool up  or whatever.  And so it's hard from the outside to know that, in fact, if you look at my track record  or predictions on technical stuff, I've been damn near perfect.  It'd be really, really good on the rationality of the market.  I've been not so good.  So I didn't think Open AI was going to get to a $300 billion valuation.  I was wrong about that.  I don't think it deserves it, but it's much harder to predict what other people are  going to believe the marketing hype and whatever.  So I don't know.  One possible outcome is people look today at GPC5  and they're like, fuck, that is not AGI.  You know, Sam has been telling us  we know how to build AGI for years.  This just isn't AGI, it's nice, it's handy,  I'm glad it's cheaper, I'll use it.  But it's not AGI.  We're not gonna get to AGI next year.  Probably not gonna save as much money for various workplaces  as we thought.  Maybe it doesn't make sense to put in all this money.  And like tomorrow in video could crash.  People could look at that and say,  This was a really disappointing thing, but they probably won't.  You know, they probably just be like, okay, let's buy some, you know, people are going to use this.  Let's sell more Chet.  And like, I can't predict the human psychology and it's really the mass psychology part of how long people are going to stick to this bet.  At some point, they have to stop doing.  You can't just do like what Elon just did and you 100 acts every time your investments are like, I think I might be getting my money from it.  I think Grok 4 is 100 times the size of Grok 2.  So what is, you know, is Grock six going to be 10,000 times as big?  Like we don't have enough power and water on the planet to feel it.  Like at some point this has to run out, but I don't know what day that's going to be.  And technically, what do you think?  Technically, technically, you say you bet at the technical predictions.  Oh, I mean, I've made many.  So I predicted that hallucinations wouldn't go anywhere.  I predicted that GPT-5 would take much longer than people thought that it would and it did.  I predicted that, you know, reasoning errors would not be solved immediately and they didn't.  And I have an assay people can read called 25 predictions for 2025.  And they're almost all on target.  They're like 22 of them or 21 of them are on target so far.  And that reviews also some of the earlier predictions I made.  Like I predicted that there wouldn't be much technical note because it would be diminishing  returns because everybody would sort of converge on roughly the same place and that has  happened.  So, you know, there may be many predictions that have been correct over the years.  one prediction, one technical prediction that because people not might not have read that.  Well, I mean, the one that I made that I'm in some ways most proud of is 25 years of predicting  that illusinations are not going to go away if you just use this art.  Kind of.  I've been correct for quarter century on that.  And to be correct for a quarter century in AI is a pretty remarkable thing.  Gary, thanks so much.  We, out of time, I'm getting chased any lost words before we wrap up.  Well, those of you who are watching this in the cybersecurity world, you have to pay  enormous attention to prompt injection attacks.  They're gonna, I think ultimately clips everything else you've ever dealt with.  Everything you know and have learned about is gonna be helpful,  but this is gonna be a massive problem where you need all hands on that.  It's gonna be really huge.  Thank you so much.  Thank you.  Thanks a lot, man.  Appreciate it."
}