version: "3.8"

services:
  spark-master:
    image: ${SPARK_IMAGE_NAME}:${SPARK_IMAGE_TAG}
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"    # Spark master port
      - "8080:8080"    # Spark web UI
    networks:
      - kafka-net

  spark-worker-1:
    image: ${SPARK_IMAGE_NAME}:${SPARK_IMAGE_TAG}
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_URL}

    volumes:
      - ${PWD}/spark:/opt/spark_jobs
      - ${PWD}/scripts:/opt/scripts
      - ${PWD}/data:/data
    env_file:
      - ../../.env.development
    depends_on:
      - spark-master
    networks:
      - kafka-net

  spark-streaming:
    image: ${SPARK_IMAGE_NAME}:${SPARK_IMAGE_TAG}
    container_name: spark-streaming
    # ENTRYPOINT in your image should be ["spark-submit"]
    command:
    - "--master"
    - "${SPARK_URL}"
    - "--conf"
    - "spark.sql.session.timeZone=UTC"
    - "--conf"
    - "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"
    - "--conf"
    - "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
    - "--packages"
    - "${SPARK_PACKAGES}"
    - "/opt/spark_jobs/pipelines/streaming_user_events_pipeline.py"
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_SUBMIT_OPTS=-XX:+UseG1GC -Duser.timezone=UTC
    env_file:
      - ../../.env.development
    volumes:
      - ./data:/data
    depends_on: [spark-master, spark-worker-1]
    networks: [kafka-net]

networks:
  kafka-net:
    external: true
    name: kafka-net

